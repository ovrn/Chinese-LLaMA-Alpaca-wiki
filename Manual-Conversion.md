### å‡†å¤‡å·¥ä½œ

1. ç¡®ä¿æœºå™¨æœ‰è¶³å¤Ÿçš„å†…å­˜åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆä¾‹å¦‚7Bæ¨¡å‹éœ€è¦13-15Gï¼‰ä»¥è¿›è¡Œåˆå¹¶æ¨¡å‹æ“ä½œã€‚
2. åŠ¡å¿…ç¡®è®¤åŸºæ¨¡å‹å’Œä¸‹è½½çš„LoRAæ¨¡å‹å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ˜¯å¦ä¸[SHA256.md](./SHA256.md)æ‰€ç¤ºçš„å€¼ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•è¿›è¡Œåˆå¹¶æ“ä½œã€‚åŸç‰ˆLLaMAåŒ…å«ï¼š`tokenizer.model`ã€`tokenizer_checklist.chk`ã€`consolidated.*.pth`ã€`params.json`
3. ä¸»è¦ä¾èµ–åº“å¦‚ä¸‹ï¼ˆå¦‚æœå‡ºé—®é¢˜å°±è¯·å®‰è£…ä»¥ä¸‹æŒ‡å®šç‰ˆæœ¬ï¼‰ï¼š
   - `transformers`ï¼ˆ4.28.0æµ‹è¯•é€šè¿‡ï¼‰
   - `sentencepiece`ï¼ˆ0.1.97æµ‹è¯•é€šè¿‡ï¼‰
   - `peft`ï¼ˆ0.2.0æµ‹è¯•é€šè¿‡ï¼‰
   - pythonç‰ˆæœ¬å»ºè®®åœ¨3.9ä»¥ä¸Š

```bash
pip install transformers
pip install sentencepiece
pip install peft
```

*æ³¨æ„ï¼šæœ¬é¡¹ç›®ä¸å¯¹ä½¿ç”¨ç¬¬ä¸‰æ–¹ï¼ˆéFacebookå®˜æ–¹ï¼‰æƒé‡çš„åˆè§„æ€§å’Œæ­£ç¡®æ€§è´Ÿè´£ï¼Œä¾‹å¦‚HuggingFaceæ¨¡å‹åº“ä¸­çš„`decapoda-research/llama-7b-hf`ï¼ˆuse at your own riskï¼‰ã€‚*


### Step 1: å°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHFæ ¼å¼

è¯·ä½¿ç”¨[ğŸ¤—transformers](https://huggingface.co/docs/transformers/installation#install-from-source)æä¾›çš„è„šæœ¬[convert_llama_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)ï¼Œå°†åŸç‰ˆLLaMAæ¨¡å‹è½¬æ¢ä¸ºHuggingFaceæ ¼å¼ã€‚å°†åŸç‰ˆLLaMAçš„`tokenizer.model`æ”¾åœ¨`--input_dir`æŒ‡å®šçš„ç›®å½•ï¼Œå…¶ä½™æ–‡ä»¶æ”¾åœ¨`${input_dir}/${model_size}`ä¸‹ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åï¼Œ`--output_dir`ä¸­å°†å­˜æ”¾è½¬æ¢å¥½çš„HFç‰ˆæƒé‡ã€‚

```bash
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir path_to_original_llama_root_dir \
    --model_size 7B \
    --output_dir path_to_original_llama_hf_dir
```

### Step 2: åˆå¹¶LoRAæƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡

è¿™ä¸€æ­¥éª¤ä¼šå¯¹åŸç‰ˆLLaMAæ¨¡å‹ï¼ˆHFæ ¼å¼ï¼‰æ‰©å……ä¸­æ–‡è¯è¡¨ï¼Œåˆå¹¶LoRAæƒé‡å¹¶ç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚æ­¤å¤„å¯æœ‰ä¸¤ç§é€‰æ‹©ï¼š

- è¾“å‡ºPyTorchç‰ˆæœ¬æƒé‡ï¼ˆ`.pth`æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨`merge_llama_with_chinese_lora.py`è„šæœ¬
  - ä»¥ä¾¿[ä½¿ç”¨llama.cppå·¥å…·è¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²](#llamacppé‡åŒ–éƒ¨ç½²)

- è¾“å‡ºHuggingFaceç‰ˆæœ¬æƒé‡ï¼ˆ`.bin`æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨`merge_llama_with_chinese_lora_to_hf.py`è„šæœ¬ï¼ˆæ„Ÿè°¢@sgsdxzy æä¾›ï¼‰
  - ä»¥ä¾¿[ä½¿ç”¨Transformersè¿›è¡Œæ¨ç†](#ä½¿ç”¨transformersæ¨ç†)
  - ä»¥ä¾¿[ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢](#ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢)


ä»¥ä¸Šä¸¤ä¸ªè„šæœ¬æ‰€éœ€å‚æ•°ä¸€è‡´ï¼Œä»…è¾“å‡ºæ–‡ä»¶æ ¼å¼ä¸åŒã€‚ä¸‹é¢ä»¥ç”ŸæˆPyTorchç‰ˆæœ¬æƒé‡ä¸ºä¾‹ï¼Œä»‹ç»ç›¸åº”çš„å‚æ•°è®¾ç½®ã€‚

```bash
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --output_dir path_to_output_dir 
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼ˆStep 1ç”Ÿæˆï¼‰
- `--lora_model`ï¼šä¸­æ–‡LLaMA/Alpaca LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨[ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°](#Model-Hub)
- `--output_dir`ï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º`./`
- ï¼ˆå¯é€‰ï¼‰`--offload_dir`ï¼šå¯¹äºä½å†…å­˜ç”¨æˆ·éœ€è¦æŒ‡å®šä¸€ä¸ªoffloadç¼“å­˜è·¯å¾„
