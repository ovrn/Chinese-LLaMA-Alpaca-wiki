The entire training process includes three parts: vocabulary expansion, pre-training, and instruction fine-tuning. Pleas refer to the [merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/merge_tokenizers.py) for vocabulary expansion; refer to the [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) in ðŸ¤—transformers and the relevant parts of dataset processing in the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) project for pre-training and self-instruct fine-tuning.

### Preparation: Vocabulary Expansion

Due to the limited support for Chinese (and other non-English languages) in the original LLaMA,

- We further expanded the Chinese vocabulary based on training with the general Chinese corpus using [sentencepiece](https://github.com/google/sentencepiece) to create a 20K Chinese vocabulary, which was then merged with the original LLaMA model's 32K vocabulary. 
- After removing duplicate tokens, the final Chinese LLaMA vocabulary size is 49,953.
- It should be noted that during the fine-tuning stage, Alpaca has one more pad token than LLaMA, so the Chinese Alpaca vocabulary size is 49,954.

For more information on the motivation behind expanding the Chinese vocabulary, please refer to the [FAQ](#FAQ).

If you want to know the details of vocabulary expansion, or expand LLaMA tokenizer with your custom vocabulary, please check [merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/merge_tokenizers.py). The script can be run as follows:

```
python merge_tokenizers.py \
  --llama_tokenizer_dir llama_tokenizer_dir \
  --chinese_sp_model_file chinese_sp_model_file
```
where
* `llama_tokenizer_dir`: path to the directory that stores the original LLaMA tokenizer
* `chinese_sp_model_file`: the Chinese sentencepiece model file generated by sentencepiece

We also release the 20K-vocab Chinese sentencepiece model that was used in vocabulary expansion, available at [scripts/chinese_sp.model](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/chinese_sp.model).