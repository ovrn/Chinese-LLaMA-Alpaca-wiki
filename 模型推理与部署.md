本项目中的模型主要支持以下推理和部署方式：

### llama.cpp
提供了一种模型量化和在本地CPU上部署方式。

教程：[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp量化部署](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp量化部署)

### 🤗Transformers
提供原生transformers推理接口，支持CPU/GPU上进行模型推理。

教程：[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用Transformers推理](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用Transformers推理)

### text-generation-webui
提供了一种可实现前端UI界面的部署方式。

教程：[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用text-generation-webui搭建界面](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用text-generation-webui搭建界面)

### LlamaChat
提供了一种基于macOS系统的图形交互界面，支持GGML（`.bin`格式）和PyTorch（`.pth`格式）版本的模型加载。

教程：[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用LlamaChat图形界面（macOS）](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用LlamaChat图形界面（macOS）)