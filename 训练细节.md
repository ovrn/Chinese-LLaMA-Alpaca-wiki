æ•´ä¸ªè®­ç»ƒæµç¨‹åŒ…æ‹¬è¯è¡¨æ‰©å……ã€é¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒä¸‰éƒ¨åˆ†ï¼Œå…¶ä¸­è¯è¡¨æ‰©å……çš„ä»£ç å‚è§[merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/merge_tokenizers.py)ï¼›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒä»£ç å‚è€ƒäº†ğŸ¤—transformersä¸­çš„[run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)å’Œ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é¡¹ç›®ä¸­æ•°æ®é›†å¤„ç†çš„ç›¸å…³éƒ¨åˆ†ã€‚

### å‡†å¤‡å·¥ä½œï¼šè¯è¡¨æ‰©å……

ç”±äºåŸç‰ˆLLaMAå¯¹ä¸­æ–‡çš„æ”¯æŒéå¸¸æœ‰é™ï¼Œæœ¬é¡¹ç›®åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å……äº†ä¸­æ–‡è¯è¡¨ã€‚

- åœ¨é€šç”¨ä¸­æ–‡è¯­æ–™ä¸Šè®­ç»ƒäº†åŸºäº[sentencepiece](https://github.com/google/sentencepiece)çš„20Kä¸­æ–‡è¯è¡¨å¹¶ä¸åŸç‰ˆLLaMAæ¨¡å‹çš„32Kè¯è¡¨è¿›è¡Œåˆå¹¶
- æ’é™¤é‡å¤çš„tokenåï¼Œå¾—åˆ°çš„æœ€ç»ˆä¸­æ–‡LLaMAè¯è¡¨å¤§å°ä¸º**49953**
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨fine-tuneé˜¶æ®µAlpacaæ¯”LLaMAå¤šä¸€ä¸ªpad tokenï¼Œæ‰€ä»¥ä¸­æ–‡Alpacaçš„è¯è¡¨å¤§å°ä¸º**49954**

æ›´å¤šå…³äºä¸­æ–‡è¯è¡¨æ‰©å……çš„åŠ¨æœºï¼Œå¯å‚è€ƒ[FAQ](#FAQ)ã€‚

å¦‚æœæ¬²äº†è§£æ‰©å……è¯è¡¨çš„å…·ä½“æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨è‡ªå·±çš„è¯è¡¨å¯¹LLaMA tokenizerè¿›è¡Œæ‰©å……ï¼Œæˆ‘ä»¬æä¾›äº†ä»£ç [merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/merge_tokenizers.py)ä¾›å‚è€ƒã€‚
è¯¥è„šæœ¬è¿è¡Œæ–¹å¼å¦‚ä¸‹ï¼š
```
python merge_tokenizers.py \
  --llama_tokenizer_dir llama_tokenizer_dir \
  --chinese_sp_model_file chinese_sp_model_file
```
å…¶ä¸­
* `llama_tokenizer_dir`:æŒ‡å‘å­˜æ”¾åŸç‰ˆLLaMA tokenizerçš„ç›®å½•
* `chinese_sp_model_file`:æŒ‡å‘ç”¨sentencepieceè®­ç»ƒçš„ä¸­æ–‡è¯è¡¨æ–‡ä»¶

æˆ‘ä»¬æ‰€ä½¿ç”¨çš„åœ¨ä¸­æ–‡é€šç”¨è¯­æ–™ä¸Šè®­ç»ƒçš„20Kä¸­æ–‡è¯è¡¨ä¹Ÿä¸€å¹¶æ”¾å‡ºï¼Œå¯ä»¥åœ¨[scripts/chinese_sp.model](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/chinese_sp.model)ä¸‹è½½ã€‚

### é¢„è®­ç»ƒ

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨çº¦20Gå·¦å³çš„é€šç”¨ä¸­æ–‡è¯­æ–™ï¼ˆä¸[ä¸­æ–‡BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)ã€[MacBERT](https://github.com/ymcui/MacBERT)ã€[LERT](https://github.com/ymcui/LERT)ã€[PERT](https://github.com/ymcui/PERT)ä¸­ä½¿ç”¨çš„è¯­æ–™ä¸€è‡´ï¼‰åœ¨åŸç‰ˆLLaMAæƒé‡çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è¿›è¡Œé¢„è®­ç»ƒã€‚è¯¥è¿‡ç¨‹åˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

1. ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“transformerå‚æ•°ï¼Œä»…è®­ç»ƒembeddingï¼Œåœ¨å°½é‡ä¸å¹²æ‰°åŸæ¨¡å‹çš„æƒ…å†µä¸‹é€‚é…æ–°å¢çš„ä¸­æ–‡è¯å‘é‡ã€‚

2. ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LoRAæŠ€æœ¯ï¼Œä¸ºæ¨¡å‹æ·»åŠ LoRAæƒé‡ï¼ˆadapterï¼‰ï¼Œè®­ç»ƒembeddingçš„åŒæ—¶ä¹Ÿæ›´æ–°LoRAå‚æ•°ã€‚

### æŒ‡ä»¤ç²¾è°ƒ

1. æŒ‡ä»¤ç²¾è°ƒé˜¶æ®µçš„ä»»åŠ¡å½¢å¼åŸºæœ¬ä¸[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ç›¸åŒã€‚è®­ç»ƒæ–¹æ¡ˆåŒæ ·é‡‡ç”¨äº†LoRAè¿›è¡Œé«˜æ•ˆç²¾è°ƒï¼Œå¹¶è¿›ä¸€æ­¥å¢åŠ äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚
2. åœ¨promptè®¾è®¡ä¸Šï¼Œç²¾è°ƒä»¥åŠé¢„æµ‹æ—¶é‡‡ç”¨çš„éƒ½æ˜¯åŸç‰ˆ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)ä¸å¸¦inputçš„æ¨¡ç‰ˆã€‚å¯¹äºåŒ…å«inputå­—æ®µçš„æ•°æ®ï¼Œé‡‡ç”¨`f"{instruction}+\n+{input}"`çš„å½¢å¼è¿›è¡Œæ‹¼æ¥ã€‚

### è®­ç»ƒæ•°æ®

æŒ‡ä»¤ç²¾è°ƒé˜¶æ®µä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®ï¼Œå…¶ä¸­7Bæ¨¡å‹çº¦2Mæ•°æ®ã€13Bæ¨¡å‹çº¦3Mæ•°æ®ã€‚åŸºæœ¬æ„æˆå¦‚ä¸‹ï¼š

| æ•°æ®                 | é‡çº§ |                             æ¥æº                             | è¯´æ˜                                                  |
| -------------------- | :--: | :----------------------------------------------------------: | ----------------------------------------------------- |
| ä¸­è‹±ç¿»è¯‘æ•°æ®         | 500K | [å¤–éƒ¨é“¾æ¥](https://github.com/brightmart/nlp_chinese_corpus#5ç¿»è¯‘è¯­æ–™translation2019zh) | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·+è§„åˆ™ç­›é€‰                 |
| pCLUEæ•°æ®            | 300K |      [å¤–éƒ¨é“¾æ¥](https://github.com/CLUEbenchmark/pCLUE)      | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·+è§„åˆ™ç­›é€‰                 |
| Alpacaæ•°æ®ï¼ˆè‹±ï¼‰     | 50K  |   [å¤–éƒ¨é“¾æ¥](https://github.com/tatsu-lab/stanford_alpaca)   | æ–¯å¦ç¦åŸç‰ˆAlpacaè®­ç»ƒæ•°æ®                              |
| Alpacaæ•°æ®ï¼ˆä¸­ï¼‰     | 50K  |                    **[æœ¬åœ°é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/data)**                    | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£å°†è‹±æ–‡ç‰ˆç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆç­›æ‰ä¸€éƒ¨åˆ†ï¼‰ |
| Self-instructionæ•°æ® | 1~2M |                           ï¼ˆæš‚æ— ï¼‰                           | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£è¿›è¡Œçˆ¬å–ï¼Œå…·ä½“è§ä»¥ä¸‹è„šæœ¬æè¿°     |

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªåŠ¨æ€ç”Ÿæˆä¸åŒé¢†åŸŸå’ŒæŒ‡ä»¤ç±»å‹çš„promptçˆ¬å–è„šæœ¬`script/crawl_prompt.py`ã€‚

```bash
python script/crawl_prompt.py output-file
```
- æ€è·¯ä¸[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)ä¸­çš„åšæ³•åŸºæœ¬ä¸€è‡´ï¼Œä¸€æ¬¡æ‰¹é‡ç”Ÿæˆ20ç»„æ•°æ®ï¼ˆå¯è‡ªè¡Œä¿®æ”¹æ¨¡æ¿ï¼‰ï¼Œä»¥é™ä½çˆ¬å–æˆæœ¬
- ç”Ÿæˆçš„æ–‡ä»¶åŒ…å«é€šè¿‡`gpt-3.5-turbo`çˆ¬å–çš„æ•°æ®ï¼ˆä½ å¿…é¡»æ‹¥æœ‰OpenAI API keyæ‰å¯ä»¥ä½¿ç”¨ï¼‰
- è™½ç„¶æŒ‡ä»¤æ¨¡æ¿ä¸­è¦æ±‚è¾“å‡ºJSONï¼Œä½†ç³»ç»Ÿå¹¶ä¸æ€»æ˜¯ä¼šè¿”å›åˆæ³•çš„JSONï¼Œéœ€è¦è‡ªè¡Œå¯¹è¿”å›æ•°æ®è¿›è¡Œæ¸…æ´—
- ç”±äºçˆ¬å–æ—¶é—´æ¯”è¾ƒé•¿ï¼Œå»ºè®®åå°è¿è¡Œè¯¥è„šæœ¬ã€‚å¤šçº¿ç¨‹è¿è¡Œæ—¶æ³¨æ„[OpenAI APIçš„è°ƒç”¨é™åˆ¶ä¸Šé™](https://platform.openai.com/docs/guides/rate-limits/overview)

### å®éªŒé…ç½®

| å®éªŒè®¾ç½®                 | é¢„è®­ç»ƒ-ç¬¬ä¸€é˜¶æ®µ  | é¢„è®­ç»ƒ-ç¬¬äºŒé˜¶æ®µ  |     æŒ‡ä»¤ç²¾è°ƒ     |
| :----------------------- | :--------------: | :--------------: | :--------------: |
| Batch Size               |       1024       |       1024       |       512        |
| Initial Learning Rate    |       2e-4       |       1e-4       |       1e-4       |
| Training Steps           |        3K        |        6K        |      6K-10K      |
| Max Length               |       512        |       512        |       512        |
| Trainable Parameters (%) |      2.97%       |      6.06%       |      6.22%       |
| Training Device          |     8 Ã— A100     |    16 Ã— A100     |    16 Ã— A100     |
| Distributed Training     | DeepSpeed Zero-2 | DeepSpeed Zero-2 | DeepSpeed Zero-2 |
