
以下给出了中文LLaMA和Alpaca模型的基本对比以及建议使用场景（包括但不限于）。

| 对比项                | 中文LLaMA                                              | 中文Alpaca                                                   |
| --------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| 训练方式              | 传统CLM （在通用语料上训练）                            | 指令精调 （在指令数据上训练）                                                     |
| 输入模板              | 不需要                                                 | 需要符合模板要求（llama.cpp/LlamaChat/inference_hf.py等已内嵌）              |
| 适用场景 ✔️            | 文本续写：给定上文，让模型继续写下去                   | 1、指令理解（问答、写作、建议等）<br/>2、多轮上下文理解（聊天等） |
| 不适用场景 ❌          | 指令理解 、多轮聊天等                                  |  文本无限制自由生成                                                       |
| llama.cpp             | 使用`-p`参数指定上文                                   | 使用`-ins`参数启动指令理解+聊天模式                          |
| text-generation-webui |  不适合chat模式                              |    使用`--cpu`可在无显卡形式下运行，若生成内容不满意，建议修改prompt                                                          |
| LlamaChat             | 加载模型时选择"LLaMA"                                  | 加载模型时选择"Alpaca"                                       |
| inference_hf.py     | 无需添加额外启动参数 | 启动时添加参数 `--with_prompt`        |
| 已知问题              | 如果不控制终止，则会一直写下去，直到达到输出长度上限。 | 目前版本模型生成的文本长度相对短一些，比较惜字如金。         |

**如果出现了模型回答质量低、胡言乱语、不理解问题等情况，请检查是否针对场景使用了正确的模型和正确的启动参数**