We mainly provide the following ways for inference and local deployment.

### llama.cpp
A tool for quantizing model and deploying on local CPU

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp-Deployment](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp-Deployment)

### ðŸ¤—Transformers
Original transformers inference method, support CPU/GPU

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Inference-with-Transformers](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Inference-with-Transformers)

### text-generation-webui
A tool for deploying model as a web UI.

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/text-generation-webui](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/text-generation-webui)

### LlamaChat

LlamaChat is a macOS app that allows you to chat with LLaMA, Alpaca, etc. Support GGML(.bin) and PyTorch (.pth) formatsã€‚

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Using-LlamaChat-Interface](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Using-LlamaChat-Interface)