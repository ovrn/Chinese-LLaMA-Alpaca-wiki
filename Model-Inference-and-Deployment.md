We mainly provide the following three ways for inference and local deployment.

### llama.cpp
A tool for quantizing model and deploying on local CPU

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp-Deployment](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp-Deployment)

### ğŸ¤—Transformers
Original transformers inference method, support CPU/GPU

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Inference-with-Transformers](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/Inference-with-Transformers)

### text-generation-webui
A tool for deploying model as a web UI.

Link: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/text-generation-webui](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/text-generation-webui)

### LlamaChat

æä¾›äº†ä¸€ç§åŸºäºmacOSç³»ç»Ÿçš„å›¾å½¢äº¤äº’ç•Œé¢ï¼Œæ”¯æŒGGMLï¼ˆ.binæ ¼å¼ï¼‰å’ŒPyTorchï¼ˆ.pthæ ¼å¼ï¼‰ç‰ˆæœ¬çš„æ¨¡å‹åŠ è½½ã€‚

æ•™ç¨‹ï¼š[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/ä½¿ç”¨LlamaChatå›¾å½¢ç•Œé¢ï¼ˆmacOSï¼‰](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8LlamaChat%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%EF%BC%88macOS%EF%BC%89)