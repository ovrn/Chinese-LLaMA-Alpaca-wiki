### 问题1：为什么不能放出完整版本权重？

答：这个问题前面已经反复强调过了。LLaMA模型的开源协议不允许我们这么做，所以相关衍生工作都在寻找可以绕过限制的方法。请相信我们设置这么多步骤，不是为了增加大家的工作量，而是客观情况所致。后续待Facebook完全开放权重之后，我们会第一时间将完整版模型以及直接可加载的量化模型放出来。在此期间，我们也会密切关注其他LLaMA相关的repo，看看有没有更好的方法。

### 问题2：后面会有33B、65B的版本吗？

答：33B和65B版本需要看情况。我们希望模型能够“健康地”增长，而不只是追求更大的模型。


### 问题3：一些任务上效果不好！

答：这里有几个可能的原因，1）本身LLaMA对中文支持不是很好，大多数相关衍生工作是直接在原版上进行pretrain/finetune的，而我们采取了更大胆的策略——增加中文词表，可能进一步加剧中文训练不充分的问题，但从长远看是否有利于后续进一步预训练就得靠时间检验了；2）指令数据的质量有待进一步提升；3）训练时间、超参等方面还有很大调整空间；4）没有RLHF；5）4-bit量化后效果可能会下降，因此可以尝试加载FP16模型，效果相对更好一些（也更慢）。

### 问题4：为什么要扩充词表？直接在原版LLaMA上用中文预训练不行吗？

答：原版LLaMA模型的词表大小是32K，其主要针对英语进行训练（具体详见[LLaMA论文](https://arxiv.org/abs/2302.13971v1)），对多语种支持不是特别理想（可以对比一下多语言经典模型XLM-R的词表大小为250K）。通过初步统计发现，LLaMA词表中仅包含很少的中文字符，所以在切词时会把中文切地更碎，需要多个byte token才能拼成一个完整的汉字，进而导致信息密度降低。比如，在扩展词表后的模型中，单个汉字倾向于被切成1个token，而在原版LLaMA中可能就需要2-3个才能组合成一个汉字，显著降低编解码的效率。

### 问题5：回复内容很短

答：目前已发现4-bit量化的模型相对FP16的模型更倾向于给出短答案。可以在prompt中命令输出长回复，比如”请详细说明……“等。其余可能的原因包括训练数据分布、训练参数、解码参数等。

### 问题6：Windows下，模型无法理解中文、生成速度很慢等问题

答： Windows用户出现模型无法理解中文、生成速度慢时，请参考以下的解决方案。

关于无法理解中文：

- llama.cpp在[PR#840](https://github.com/ggerganov/llama.cpp/pull/840)解决了这个问题，请优先使用最新版llama.cpp。如果还不能解决建议查看下面的方案。
- [Unicode (Windows) Support for llama.cpp](https://github.com/josStorer/llama.cpp-unicode-windows)（感谢@josStorer开发）
- [#issue 11](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues/11)（感谢@LainNya、@boholder、@hyperzlib 等人提供解决方案）

关于生成速度很慢：

- [#issue 51](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues/51)（感谢@wscsjnhboy 提供解决方案）

### 问题7：Chinese-LLaMA 13B模型没法用llama.cpp启动，提示维度不一致

答：这与13B模型拆分成了两个文件，每个文件大小不相同有关，见[Issue#133](https://github.com/ymcui/Chinese-LLaMA-Alpaca/issues/133)。动手能力强的用户可以用该issue提到的方法自己尝试解决。另一方面，Chinese-LLaMA模型本身并不是为对话、交互设计，而是为进一步在中文指令精调或其他任务精调提供基底，因此也并不建议用llama.cpp加载Chinese-LLaMA模型。